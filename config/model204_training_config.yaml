logging:
    use_wandb: True
    wandb:
        project: 'commonlit'
        group: 'general'
dataset:
    use_summary_text: True
    use_prompt_title: True
    use_prompt_question: False
    use_prompt_text:  False
    use_current_data_true_labels: True
    use_previous_data_pseudo_labels: False
    use_current_data_pseudo_labels: False
    current_data_pseudo_version: ''
    previous_data_pseudo_version: ''
    check_cv_on_all_data: False
    train_print_frequency: 150
    valid_print_frequency: 50
    target_cols: ['content', 'wording']
    n_folds: 4
    max_length: 1024
    set_max_length_from_data: True
architecture:
    model_name: 'microsoft/deberta-v3-large'
    pretrained: True
    from_checkpoint: False
    checkpoint_id: ''
    backbone_config_path: ''
    hidden_dropout: 0.
    hidden_dropout_prob: 0.
    attention_dropout: 0.
    attention_probs_dropout_prob: 0.
    pooling_type: 'ConcatPooling' # ['MeanPooling', 'ConcatPooling', 'WeightedLayerPooling', 'GRUPooling', 'LSTMPooling', 'AttentionPooling']
    gru_pooling:
        hidden_size: 1024
        dropout_rate: 0.1
        bidirectional: False
    weighted_pooling:
        layer_start: 4
        layer_weights: null
    wk_pooling:
        layer_start: 4
        context_window_size: 2
    lstm_pooling:
        hidden_size: 1024
        dropout_rate: 0.1
        bidirectional: False
    attention_pooling:
        hiddendim_fc: 1024
        dropout: 0.1
    concat_pooling:
        n_layers: 3
    gradient_checkpointing: True
    freeze_embeddings: False
    freeze_n_layers: 0
    reinitialize_n_layers: 2
optimizer:
    use_swa: False
    swa:
        swa_start: 2
        swa_freq: 10
        swa_lr: 1e-6

    adamw_lr: 5.0e-5
    warmup_ratio: 0.1
    encoder_lr: 0.00002 
    embeddings_lr: 0.00002
    decoder_lr: 0.00002
    group_lt_multiplier: 1
    n_groups: 1
    eps: 1.e-6
    betas: [0.9, 0.999]
    weight_decay: 0.01
scheduler:
    scheduler_type: 'cosine_schedule_with_warmup' # [constant_schedule_with_warmup, linear_schedule_with_warmup, cosine_schedule_with_warmup,polynomial_decay_schedule_with_warmup]
    batch_scheduler: True
    constant_schedule_with_warmup:
        n_warmup_steps: 0
    linear_schedule_with_warmup:
        n_warmup_steps: 0
    cosine_schedule_with_warmup:
        n_cycles: 0.5
        n_warmup_steps: 100
    polynomial_decay_schedule_with_warmup:
        n_warmup_steps: 0
        power: 1.0
        min_lr: 0.0

adversarial_learning:
    adversarial_lr: 0.0001
    adversarial_eps: 0.005
    adversarial_epoch_start: 99

training:
    apex: True
    epochs: 10
    precision: 16
    val_check_interval: 2
    train_batch_size: 16
    valid_batch_size: 16
    gpu_count: 1
    multi_dropout: True
    gradient_accumulation_steps: 1
    max_grad_norm: 1000
    unscale: False
    cutmix_ratio: 0.5
    mask_ratio: 0.25
    train_print_frequency: 100
    valid_print_frequency: 50
    swa: 
        swa_lrs: 5.0e-4
        swa_epoch_start: 4
        annealing_epochs: 2
        annealing_strategy: 'cos' # 'linear'

es: 
    patience: 15
    mode: "min"
    min_delta: 1.0e-10
environment: 
    seed: 42
    n_workers: 4
criterion:
    criterion_type: 'MSELoss' # ['SmoothL1Loss', 'MSELoss', 'RMSELoss', 'MCRMSELoss',]
    smooth_l1_loss:
        beta: 0.1
        reduction: 'mean'
    mse_loss:
        reduction: 'mean'
    rmse_loss:
        eps: 1.e-9
        reduction: 'mean'
    mcrmse_loss:
        weights: [0.5, 0.5]

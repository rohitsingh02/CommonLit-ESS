dataset_class: commonlit_dataset
model_class: commonlit_model
experiment_name: model1
output_dir: ../output
debug: false
overlap_threshold: 0.7
swa_model: true
architecture:
    reinit_n_layers: 0
    mixout: 0.0
    pretrained_weights: "" 
    model_name: microsoft/deberta-v3-large #./pretrained/deberta-large  #deepset/deberta-v3-large-squad2  #microsoft/deberta-v3-base 
    custom_intermediate_dropout: true
    dropout: 0
    gradient_checkpointing: true
    intermediate_dropout: 0
    pool: Mean # GeM # WLP # Mean
dataset:
    target_cols:
    - 'content'
    - 'wording'
    replace_newline: true
    max_len: 1600 #96 #128
    fold: 0
    base_dir: ../input
    num_folds: 4
environment:
    num_jobs: 12
    mixed_precision: true
    num_workers: 4
    seed: 42
training:
    fold: -1
    swa_start: 0
    # preferred_f1: fb1
    lr: 1.0e-5 # lowered lr from 1.0e-5
    save_oofs: true
    batch_size: 1 #3
    fp16: true
    accumulation_steps: 1
    epochs: 10
    print_freq: 2000
    batch_scheduler: true
    grad_accumulation: 1
    max_grad_norm: 0.012
es: 
    patience: 10
    delta: 0.001
wandb: 
    enable: false
    project_name: shovel_ready
awp:
    enable: false
    start_epoch: 0